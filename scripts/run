#!/usr/bin/env python
import argparse
import warnings
from pathlib import Path
from typing import Union

import gif
import jax
import jax.numpy as jnp
import moviepy as mp
from gymnasium import Env, make
from gymnasium.error import NameNotFound
from jax.lib import xla_bridge
from matplotlib._api.deprecation import MatplotlibDeprecationWarning
from tqdm import tqdm
import flax
import pickle
from jax.profiler import trace
from PIL import Image

import optax

import gym_cellular_automata as gymca
from gym_cellular_automata.agents.jax_ppo import (
    run_rollout_loop,
    load_actor,
    Network,
    Actor,
    Critic,
    TrainState,
)
from gym_cellular_automata.forest_fire.bulldozer.utils.advanced_bulldozer_render import (
    render,
    plot_grid_attribute,
)
from multiprocessing import Pool
from functools import partial
from collections import defaultdict
import matplotlib
import numpy as np
from gym_cellular_automata.agents.args import (
    Args,
    PPOArgs,
    EnvArgs,
    VisualizationArgs,
    ExperimentArgs,
)

DEFAULT_UPDATES = 40
DEFAULT_MILISECOND_FRAME = 80

matplotlib.use("agg")
# Disable JAX JIT compilation
# jax.config.update("jax_disable_jit", True)
# print("JAX JIT compilation has been disabled.")


# Add at the beginning of the file, after imports
def get_default_device():
    """Get the default JAX device (GPU/TPU if available, else CPU)"""
    return xla_bridge.get_backend().platform


# Set default device platform
DEFAULT_PLATFORM = get_default_device()
print(f"Default platform: {DEFAULT_PLATFORM}")

# Configure JAX to use the default platform
jax.config.update("jax_platform_name", DEFAULT_PLATFORM)


@gif.frame
def process_single_frame(args, empty, tree, fire, title, pos_fires):
    """Helper function that applies the decorator inside the process"""

    grid = args["grid"]
    time_per_frame = args["time"]
    position = args["position"]
    wind_index = args["wind_index"]
    game_idx = args["game_index"]
    frame_idx = args["frame_index"]
    cell_count = args["cell_count"]
    is_night = args["is_night"]
    pos_fire = pos_fires[game_idx]
    dousing_count = args["dousing_count"][game_idx]

    # return make_frame_local(env, grid, time_per_frame, position, pos_fire, wind_index)
    plot = render(
        empty,
        tree,
        fire,
        title,
        grid,
        time_per_frame,
        position,
        cell_count,
        pos_fire,
        dousing_count,
        wind_index,
        is_night,
    )
    # plt.close("all")
    return plot


@gif.frame
def make_frame(plot):
    return plot


def get_run(env: Env, actor):

    warnings.filterwarnings("ignore", category=MatplotlibDeprecationWarning)

    # @gif.frame
    def run(obs, info, episode_stats, past_obs, iteration):
        # if save_gif:
        #     env.render()
        action = actor(obs[0], obs[1]["position"], iteration)
        print("Action", action)

        step_tuple = env.stateless_step(action, obs, info)
        (
            next_obs,
            reward,
            next_done,
            truncated,
            next_info,
        ) = step_tuple
        print(f"Reward: {reward}")
        print(f"Time Step: {next_obs[1]['per_env_context']['time_step']}")
        # print(f"Day Length: {next_obs[1]['per_env_context']['current_day_length']}")

        # Create PIL Image from RGB array
        # rgb_array = np.array(next_obs[0][0])
        # pil_image = Image.fromarray(np.uint8(rgb_array))
        # pil_image.save("test.png")

        new_episode_return = episode_stats.episode_returns + next_info["reward"]
        new_episode_length = episode_stats.episode_lengths + 1

        episode_stats = episode_stats.replace(
            episode_returns=(new_episode_return)
            * (1 - next_info["terminated"])
            * (1 - next_info["TimeLimit.truncated"]),
            episode_lengths=(new_episode_length)
            * (1 - next_info["terminated"])
            * (1 - next_info["TimeLimit.truncated"]),
            # only update the `returned_episode_returns` if the episode is done
            returned_episode_returns=jnp.where(
                next_info["terminated"] + next_info["TimeLimit.truncated"],
                new_episode_return,
                episode_stats.returned_episode_returns,
            ),
            returned_episode_lengths=jnp.where(
                next_info["terminated"] + next_info["TimeLimit.truncated"],
                new_episode_length,
                episode_stats.returned_episode_lengths,
            ),
        )

        new_episode_return = episode_stats.episode_returns + next_info["reward"]
        if jnp.any(next_info["terminated"]):
            import pdb

            pdb.set_trace()
        step_tuple = env.conditional_reset(step_tuple, action)

        if next_done.any():
            print(next_obs[1]["position"])
            print(next_obs[1]["time"])
        return step_tuple, episode_stats

    return run


@flax.struct.dataclass
class EpisodeStatistics:
    episode_returns: jnp.array
    episode_lengths: jnp.array
    returned_episode_returns: jnp.array
    returned_episode_lengths: jnp.array


def process_agent_obs_func(x):
    """Process a single frame into a PIL Image and resize it 8x larger"""
    rgb_array = np.array(x["agent_observation_grid"])
    pil_image = Image.fromarray(np.uint8(rgb_array))
    width, height = pil_image.size
    resized_image = pil_image.resize((width * 8, height * 8))
    return resized_image


def save_frames_and_plots(
    env,
    name,
    num_envs,
    frames_per_game,
    duration_between,
    no_hidden=False,
    parallel=False,
):
    def get_paths(key: str, i: int, key_name: str = "") -> str:
        paths = {}
        if isinstance(name, str):
            # Create main gifs directory
            base_folder = Path().cwd() / "gifs"
            base_folder.mkdir(exist_ok=True)

            # Create env-specific subfolder
            env_folder = base_folder / name
            env_folder.mkdir(exist_ok=True)
            for j in range(num_envs):
                index_folder = base_folder / env_folder / f"{name}_{j}"
                index_folder.mkdir(exist_ok=True)

            # Set paths for all files
            paths = {
                "gif": base_folder
                / env_folder
                / f"{name}_{i}"
                / f"{name}_{key_name}_{i}.gif",
                "mp4": base_folder
                / env_folder
                / f"{name}_{i}"
                / f"{name}_{key_name}_{i}.mp4",
                "altitude": base_folder
                / env_folder
                / f"{name}_{i}"
                / f"{name}_{key}_{i}_altitude.png",
                "density": base_folder
                / env_folder
                / f"{name}_{i}"
                / f"{name}_{key}_{i}_density.png",
                "vegitation": base_folder
                / env_folder
                / f"{name}_{i}"
                / f"{name}_{key}_{i}_vegitation.png",
            }
        return paths[key]

    if name == "advanced_bulldozer" and not no_hidden:
        # Generate and save altitude plots
        altitude_plots = env.altitude_render()
        for i in range(num_envs):
            altitude_plots[i].gcf().savefig(get_paths("altitude", i))
            altitude_plots[i].close()

        # Generate and save density plots
        density_plots = env.density_render()
        for i in range(num_envs):
            density_plots[i].gcf().savefig(get_paths("density", i))
            density_plots[i].close()

        # Generate and save vegetation plots
        vegetation_plots = env.vegitation_render()
        for i in range(num_envs):
            vegetation_plots[i].gcf().savefig(get_paths("vegitation", i))
            vegetation_plots[i].close()

    process_funcs = [
        # (
        #     partial(
        #         process_single_frame,
        #         empty=env._empty,
        #         tree=env._tree,
        #         fire=env._fire,
        #         title=env.spec.id if env.spec is not None else env.title,
        #         pos_fires=env._pos_fire,
        #     ),
        #     "true_grid",
        # ),
        (process_agent_obs_func, "true_grid"),
        (process_agent_obs_func, "agent_observation_grid"),
    ]

    n_processes = 8

    # Process all frames in parallel
    # Initialize empty games list with the right number of sublists

    # Flatten all frames into a single list with their game indices
    flat_frames = []
    frame_to_game_idx = []
    for game_idx, frames in enumerate(frames_per_game):
        flat_frames.extend(frames)
        frame_to_game_idx.extend([game_idx] * len(frames))

    for process_func, key in process_funcs:
        games = [[] for _ in range(len(frames_per_game))]
        if parallel:
            chunk_size = 300
            with Pool(processes=n_processes) as pool:
                for i in range(0, len(flat_frames), chunk_size):
                    chunk = flat_frames[i : i + chunk_size]
                    chunk_processed = list(
                        tqdm(
                            pool.imap(process_func, chunk),
                            total=len(chunk),
                            desc=f"Processing frames {i}-{i+len(chunk)}",
                        )
                    )
                    # Append each processed frame to its corresponding game list
                    for frame_idx, processed_frame in enumerate(chunk_processed):
                        game_idx = frame_to_game_idx[i + frame_idx]
                        games[game_idx].append(processed_frame)
        else:
            # Process frames sequentially and organize them into games
            for frame_idx, frame in enumerate(
                tqdm(
                    map(process_func, flat_frames),
                    total=len(flat_frames),
                    desc="Processing frames",
                )
            ):
                game_idx = frame_to_game_idx[frame_idx]
                games[game_idx].append(frame)

        for i in range(len(games)):
            fps = 1000 / duration_between
            clip = mp.ImageSequenceClip([np.array(img) for img in games[i]], fps=fps)
            clip.write_videofile(
                get_paths("mp4", i, key), fps=30, threads=1, codec="libx264"
            )


def generate_frames(
    env: Env,
    name: Union[str, Path],
    args: Args,
    actor=None,
    key=jax.random.key(0),
):
    """Generate frames for visualization and/or training

    Args:
        env: The environment to run
        name: Name for saving outputs
        args: Structured arguments container
        actor: Optional pre-trained actor (if None, will use random actions)
        key: JAX random key
    """
    import time

    start_time = time.time()

    run = get_run(env, actor)
    frames_per_game = [[] for _ in range(args.env.num_envs)]
    next_obs, report = env.reset()
    next_info = {
        "TimeLimit.truncated": jnp.full(args.env.num_envs, False),
        "terminated": jnp.full(args.env.num_envs, False),
        "steps_elapsed": jnp.zeros(args.env.num_envs),
        "reward_accumulated": jnp.zeros(args.env.num_envs),
        "reward": jnp.zeros(args.env.num_envs),
    }

    def run_env(next_local_obs, next_info):
        past_action_logits = jnp.zeros(args.env.num_envs, dtype=jnp.float32)
        past_obs = next_local_obs
        current_obs = next_local_obs
        episode_stats = EpisodeStatistics(
            episode_returns=jnp.zeros(args.env.num_envs, dtype=jnp.float32),
            episode_lengths=jnp.zeros(args.env.num_envs, dtype=jnp.int32),
            returned_episode_returns=jnp.zeros(args.env.num_envs, dtype=jnp.float32),
            returned_episode_lengths=jnp.zeros(args.env.num_envs, dtype=jnp.int32),
        )
        progress_bar = tqdm(range(args.viz.steps), desc="Generating frames")
        for frame_index in progress_bar:
            (
                (next_local_obs, reward, next_done, truncated, next_info),
                episode_stats,
            ) = run(next_local_obs, next_info, episode_stats, past_obs, frame_index)

            swap_obs = current_obs
            current_obs = next_local_obs
            past_obs = swap_obs

            avg_episode_length = np.mean(
                jax.device_get(episode_stats.returned_episode_lengths)
            )
            current_episode_length = np.mean(
                jax.device_get(episode_stats.episode_lengths)
            )
            progress_bar.set_postfix(
                {
                    "avg_episode_length": f"{avg_episode_length:.2f}",
                    "current_episode_length": f"{current_episode_length:.2f}",
                },
                refresh=True,
            )

            for game_index in range(next_local_obs[0].shape[0]):
                frames_per_game[game_index].append(
                    {
                        "grid": env.MDP.grid_to_rgb(
                            next_local_obs[1]["per_env_context"]["true_grid"][
                                game_index
                            ],
                            next_local_obs[1]["per_env_context"],
                            next_local_obs[1]["position"][game_index],
                        ),
                        "agent_observation_grid": next_local_obs[0][game_index],
                        "wind_index": next_local_obs[1]["per_env_context"][
                            "wind_index"
                        ][game_index],
                        "is_night": next_local_obs[1]["per_env_context"]["is_night"][
                            game_index
                        ],
                        "dousing_count": next_local_obs[1]["per_env_context"][
                            "dousing_count"
                        ][game_index],
                        "time": next_local_obs[1]["time"][game_index],
                        "position": next_local_obs[1]["position"][game_index],
                        "frame_index": frame_index,
                        "game_index": game_index,
                        "cell_count": env.count_cells(next_local_obs[0][game_index]),
                    }
                )

            avg_episodic_return = np.mean(
                jax.device_get(episode_stats.returned_episode_returns)
            )
            current_episodic_return = np.mean(
                jax.device_get(episode_stats.episode_returns)
            )
            progress_bar.set_postfix(
                {
                    "avg_return": f"{avg_episodic_return:.2f}",
                    "current_return": f"{current_episodic_return:.2f}",
                },
                refresh=True,
            )
        return next_local_obs, next_info

    if args.exp.no_train:
        if args.exp.profile:
            with trace(
                "./profile",
                create_perfetto_link=True,
                create_perfetto_trace=True,
            ):
                next_obs, next_info = run_env(next_obs, next_info)
        else:
            next_obs, next_info = run_env(next_obs, next_info)

        save_frames_and_plots(
            env,
            name,
            args.env.num_envs,
            frames_per_game,
            args.viz.duration,
            not args.env.use_hidden,
            parallel=True,
        )
    else:
        if len(jax.devices()) >= 4:
            device = jax.devices("tpu")[args.exp.device]
        else:
            device = jax.devices("cpu")[0]

        with jax.default_device(device):
            storage_returns, agent_state, run_name = run_rollout_loop(
                env,
                args=args,
                key=key,
            )

        frames_per_game = []
        if args.viz.gif:
            for video_index, video_frames in enumerate(storage_returns):
                frames_per_game.append([])
                for frame_group in video_frames:
                    grid_obs = frame_group["grid_obs"]
                    position_obs = frame_group["position_obs"]
                    contexts = frame_group["contexts"]
                    for frame_index in range(grid_obs.shape[1]):
                        frames_per_game[video_index].append(
                            {
                                "grid": contexts["true_grid"][0, frame_index],
                                "agent_observation_grid": grid_obs[0, frame_index],
                                "is_night": contexts["is_night"][0, frame_index],
                                "dousing_count": contexts["dousing_count"][
                                    0, frame_index
                                ],
                                "wind_index": contexts["wind_index"][0, frame_index],
                                "time": contexts["time"][0, frame_index],
                                "position": tuple(
                                    map(int, position_obs[0, frame_index])
                                ),
                                "frame_index": frame_index,
                                "game_index": 0,
                                "cell_count": env.count_cells(grid_obs[0, frame_index]),
                            }
                        )

            save_frames_and_plots(
                env,
                name,
                len(frames_per_game),
                frames_per_game,
                args.viz.duration,
                not args.env.use_hidden,
                parallel=not bool(len(jax.devices()) >= 4),
            )

        with open(f"runs/{run_name}_params.pkl", "wb") as f:
            pickle.dump(agent_state.params, f)

    end_time = time.time()
    print(f"Generation completed in {end_time - start_time:.2f} seconds")


def generate_gif_envs(args: Args):
    prototypes = ["helicopter", "bulldozer", "advanced_bulldozer"]
    proto_id = 2
    name = prototypes[proto_id]
    ProtoEnv = gymca.prototypes[proto_id]

    key = jax.random.key(args.exp.seed)
    key, subkey = jax.random.split(key)

    env = ProtoEnv(
        nrows=args.env.size,
        ncols=args.env.size,
        key=key,
        num_envs=args.env.num_envs,
        speed_move=args.env.speed_move * args.env.speed_multiplier,
        speed_act=0.03 * args.env.speed_multiplier,
        use_hidden=args.env.use_hidden,
        middle_fire=False,
        enable_extensions=args.env.enable_extensions,
    )
    actions = [
        3,
        3,
        3,
        3,
        3,
        3,
        7,
        7,
        7,
        7,
        7,
        7,
        3,
        3,
        3,
        3,
        3,
        3,
        3,
        5,
        5,
        5,
        5,
        5,
        5,
        5,
        7,
        7,
        7,
        7,
        7,
        7,
        7,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
        4,
    ]
    # actions_200 = [(3, 96), (7, 112), (3, 100), (5, 85), (7, 112), (4, 256)]
    actions_200 = [(3, 104), (7, 110), (3, 32), (7, 32), (5, 32), (1, 32)]
    actions_100 = [
        (3, 52),
        (7, 55),
        (3, 16),
        (7, 16),
        (5, 16),
        (1, 16),
        (5, 1),
        (1, 1),
        (4, 1),
        (3, 18),
        (7, 18),
        (5, 18),
        (1, 18),
    ]
    # actions_100 = [(3, 10), (5, 8), (3, 6)]

    def run_actions(x, y, iteration):
        if len(actions_100) == 0:
            return jnp.array([[4, 0, 0]])
        (action, count) = actions_100[0]
        count -= 1
        if count == 0:
            actions_100.pop(0)
        else:
            actions_100[0] = (action, count)

        if iteration % 2 == 0:
            return jnp.array([[action, 1, 0]])
        else:
            return jnp.array([[action, 0, 0]])

    if args.exp.params_path:
        actor = lambda x, y, iteration: load_actor(args.exp.params_path, env)
    else:
        actor = lambda x, y, iteration: env.total_action_space.sample()

    actor = run_actions

    generate_frames(
        env,
        name,
        args=args,
        actor=actor,
        key=subkey,
    )


def parse_args():
    parser = argparse.ArgumentParser(
        description="Train PPO agent on cellular automata environments"
    )

    # Environment args
    env = parser.add_argument_group("Environment")
    env.add_argument("--env-id", type=str, default="advanced_bulldozer")
    env.add_argument("--num-envs", "-n", type=int, default=8)
    env.add_argument("--size", "-z", type=int, default=256)
    env.add_argument("--speed-move", "-m", type=float, default=0.12)
    env.add_argument("--speed-multiplier", type=float, default=1)
    env.add_argument("--no-hidden", action="store_true")
    env.add_argument("--enable-extensions", action="store_true")

    # PPO args
    ppo = parser.add_argument_group("PPO")
    ppo.add_argument("--learning-rate", type=float, default=2.5e-4)
    ppo.add_argument("--anneal-lr", action="store_true", default=True)
    ppo.add_argument("--gamma", type=float, default=0.99)
    ppo.add_argument("--gae-lambda", type=float, default=0.95)
    # ... add other PPO args

    # Visualization args
    viz = parser.add_argument_group("Visualization")
    viz.add_argument("--gif", action="store_true")
    viz.add_argument("--duration", "-d", type=float, default=DEFAULT_MILISECOND_FRAME)
    viz.add_argument("--recording-times", type=int, default=8)
    viz.add_argument("--frames-per-recording", type=int, default=8)

    # Experiment args
    exp = parser.add_argument_group("Experiment")
    exp.add_argument("--exp-name", type=str, default="ppo")
    exp.add_argument("--seed", type=int, default=1)
    exp.add_argument("--track", action="store_true")
    exp.add_argument("--wandb-project", type=str, default="extended-mind")
    exp.add_argument("--wandb-entity", type=str, default="glen-berseth")
    exp.add_argument("--device", type=int, default=0)
    exp.add_argument("--profile", "-p", action="store_true")
    viz.add_argument("--steps", "-s", type=int, default=DEFAULT_UPDATES)
    exp.add_argument("--num-ppo-steps", type=int, default=128)
    exp.add_argument("--no-train", "-t", action="store_true")
    exp.add_argument("--params", type=str)

    args = parser.parse_args()
    return args


def args_to_structured_args(args) -> Args:
    """Convert flat argparse args to nested Args dataclass"""
    ppo_args = PPOArgs(
        learning_rate=args.learning_rate,
        anneal_lr=args.anneal_lr,
        gamma=args.gamma,
        gae_lambda=args.gae_lambda,
        # ... other PPO args
    )

    env_args = EnvArgs(
        env_id=args.env_id,
        num_envs=args.num_envs,
        size=args.size,
        speed_move=args.speed_move,
        speed_multiplier=args.speed_multiplier,
        use_hidden=not args.no_hidden,
        enable_extensions=args.enable_extensions,
    )

    viz_args = VisualizationArgs(
        gif=args.gif,
        steps=args.steps,
        duration=args.duration,
        recording_times=args.recording_times,
        frames_per_recording=args.frames_per_recording,
    )

    exp_args = ExperimentArgs(
        exp_name=args.exp_name,
        seed=args.seed,
        track=args.track,
        wandb_project_name=args.wandb_project,
        wandb_entity=args.wandb_entity,
        device=args.device,
        profile=args.profile,
        total_timesteps=args.steps,
        num_ppo_steps=args.num_ppo_steps,
        no_train=args.no_train,
        params_path=args.params,
    )

    return Args(ppo=ppo_args, env=env_args, viz=viz_args, exp=exp_args)


if __name__ == "__main__":
    args = parse_args()
    structured_args = args_to_structured_args(args)

    gif.options.matplotlib["dpi"] = 200
    generate_gif_envs(structured_args)
